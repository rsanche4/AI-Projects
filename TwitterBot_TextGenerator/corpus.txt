The technological advances witnessed in the computer industry are the result of a
long chain of immense and successful efforts made by two major forces. These
are the academia, represented by university research centers, and the industry,
represented by computer companies. It is, however, fair to say that the current technological advances in the computer industry owe their inception to university
research centers. In order to appreciate the current technological advances in the
computer industry, one has to trace back through the history of computers and
their development. The objective of such historical review is to understand the
factors affecting computing as we know it today and hopefully to forecast the
future of computation. A great majority of the computers of our daily use are
known as general purpose machines. These are machines that are built with no
specific application in mind, but rather are capable of performing computation
needed by a diversity of applications. These machines are to be distinguished
from those built to serve (tailored to) specific applications. The latter are known
as special purpose machines.
Computer systems have conventionally been defined through their interfaces at
a number of layered abstraction levels, each providing functional support to its predecessor. Included among the levels are the application programs, the high-level
languages, and the set of machine instructions. Based on the interface between
different levels of the system, a number of computer architectures can be defined.
The interface between the application programs and a high-level language is
referred to as a language architecture. The instruction set architecture defines the
interface between the basic machine instruction set and the runtime and I/O control.
A different definition of computer architecture is built on four basic viewpoints.
These are the structure, the organization, the implementation, and the performance.
In this definition, the structure defines the interconnection of various hardware components, the organization defines the dynamic interplay and management of the
various components, the implementation defines the detailed design of hardware
components, and the performance specifies the behavior of the computer system.
Computer architects have always been striving to increase the performance of their
architectures. This has taken a number of forms. Among these is the philosophy that
by doing more in a single instruction, one can use a smaller number of instructions to
perform the same job. The immediate consequence of this is the need for fewer
memory read/write operations and an eventual speedup of operations. It was also
argued that increasing the complexity of instructions and the number of addressing
modes has the theoretical advantage of reducing the “semantic gap” between the
instructions in a high-level language and those in the low-level (machine) language.
A single (machine) instruction to convert several binary coded decimal (BCD)
numbers to binary is an example for how complex some instructions were intended
to be. The huge number of addressing modes considered (more than 20 in the
VAX machine) further adds to the complexity of instructions. Machines following
this philosophy have been referred to as complex instructions set computers
(CISCs). Examples of CISC machines include the Intel PentiumTM, the Motorola
MC68000TM, and the IBM & Macintosh PowerPCTM.

Computer technology has shown an unprecedented rate of improvement. This
includes the development of processors and memories. Indeed, it is the advances
in technology that have fueled the computer industry. The integration of numbers
of transistors (a transistor is a controlled on/off switch) into a single chip has
increased from a few hundred to millions. This impressive increase has been
made possible by the advances in the fabrication technology of transistors.
Performance analysis should help answering questions such as how fast can a
program be executed using a given computer? In order to answer such a question,
we need to determine the time taken by a computer to execute a given job. We
define the clock cycle time as the time between two consecutive rising (trailing)
edges of a periodic clock signal. Clock cycles allow counting unit computations,
because the storage of computation results is synchronized with rising (trailing) clock edges.
The time required to execute a job by a computer is often expressed in terms of clock cycles.
While MIPS measures the rate of average instructions, MFLOPS is only defined for
the subset of floating-point instructions. An argument against MFLOPS is the fact
that the set of floating-point operations may not be consistent across machines
and therefore the actual floating-point operations will vary from machine to
machine. Yet another argument is the fact that the performance of a machine for
a given program as measured by MFLOPS cannot be generalized to provide a
single performance metric for that machine.
In order to be able to move a word in and out of the memory, a distinct address
has to be assigned to each word. This address will be used to determine the location
in the memory in which a given word is to be stored. This is called a memory write
operation. Similarly, the address will be used to determine the memory location
from which a word is to be retrieved from the memory. This is called a memory
read operation.
Information involved in any operation performed by the CPU needs to be addressed.
In computer terminology, such information is called the operand. Therefore, any
instruction issued by the processor must carry at least two types of information.
These are the operation to be performed, encoded in what is called the op-code
field, and the address information of the operand on which the operation is to be
performed, encoded in what is called the address field.
Instructions can be classified based on the number of operands as: three-address,
two-address, one-and-half-address, one-address, and zero-address. We explain
these classes together with simple examples in the following paragraphs. It should
be noted that in presenting these examples, we would use the convention operation,
source, destination to express any instruction. In that convention, operation represents the operation to be performed, for example, add, subtract, write, or read.
The source field represents the source operand(s). The source operand can be a constant, a value stored in a register, or a value stored in the memory. The destination
field represents the place where the result of the operation is to be stored, for
example, a register or a memory location.
Control (sequencing) instructions are used to change the sequence in which
instructions are executed. They take the form of conditional branching
(conditional jump), unconditional branching (jump), or call
instructions. A common characteristic among these instructions is that their
execution changes the program counter (PC) value. The change made in the PC
value can be unconditional, for example, in the unconditional branching or the
jump instructions. In this case, the earlier value of the PC is lost and execution of
the program starts at a new value specified by the instruction. Consider, for example,
the instruction jump new-address. Execution of this instruction will cause the
PC to be loaded with the memory location represented by NEW-ADDRESS
whereby the instruction stored at this new address is executed.
The CALL instructions are used to cause execution of the program to transfer to a
subroutine. A call instruction has the same effect as that of the JUMP in terms of
loading the PC with a new value from which the next instruction is to be executed.
However, with the call instruction the incremented value of the PC (to point to the
next instruction in sequence) is pushed onto the stack. Execution of a RETURN
instruction in the subroutine will load the PC with the popped value from the
stack. This has the effect of resuming program execution from the point where
branching to the subroutine has occurred.
Input and output instructions (I/O instructions) are used to transfer data between the
computer and peripheral devices. The two basic I/O instructions used are the INPUT
and OUTPUT instructions. The INPUT instruction is used to transfer data from an
input device to the processor. Examples of input devices include a keyboard or a
mouse. Input devices are interfaced with a computer through dedicated input
ports. Computers can use dedicated addresses to address these ports. Suppose that
the input port through which a keyboard is connected to a computer carries the
unique address 1000. Therefore, execution of the instruction INPUT 1000 will
cause the data stored in a specific register in the interface between the keyboard
and the computer, call it the input data register, to be moved into a specific register
(called the accumulator) in the computer. Similarly, the execution of the instruction
OUTPUT 2000 causes the data stored in the accumulator to be moved to the data
output register in the output device whose address is 2000.
A typical CPU has three major components: (1) register set, (2) arithmetic logic
unit (ALU), and (3) control unit (CU). The register set differs from one computer
architecture to another. It is usually a combination of general-purpose and specialpurpose registers. General-purpose registers are used for any purpose, hence the
name general purpose. Special-purpose registers have specific functions within
the CPU. For example, the program counter (PC) is a special-purpose register
that is used to hold the address of the instruction to be executed next. Another
example of special-purpose registers is the instruction register (IR), which is
used to hold the instruction that is currently executed. The ALU provides the circuitry needed to perform the arithmetic, logical and shift operations demanded of
the instruction set. The control unit is
the entity responsible for fetching the instruction to be executed from the main
memory and decoding and then executing it.
The CPU fetches instructions from memory, reads and writes data from and to
memory, and transfers data from and to input/output devices.
The execution cycle is repeated as long as there are more instructions to execute.
A check for pending interrupts is usually included in the cycle. Examples of interrupts include I/O device request, arithmetic overflow, or a page fault.
When an interrupt request is encountered, a transfer to an interrupt handling routine
takes place. Interrupt handling routines are programs that are invoked to collect the
state of the currently executing program, correct the cause of the interrupt, and
restore the state of the program.
The actions of the CPU during an execution cycle are defined by micro-orders
issued by the control unit. These micro-orders are individual control signals sent
over dedicated control lines. For example, let us assume that we want to execute an
instruction that moves the contents of register X to register Y. Let us also assume
that both registers are connected to the data bus, D. The control unit will issue a control signal to tell register X to place its contents on the data bus D. After some delay,
another control signal will be sent to tell register Y to read from data bus D. The activation of the control signals is determined using either hardwired control or microprogramming.
Registers are essentially extremely fast memory locations within the CPU that are
used to create and store the results of CPU operations and other calculations. Different computers have different register sets.
They differ in the number of registers, register types, and the length of each register. They also differ in the usage of each
register. General-purpose registers can be used for multiple purposes and assigned
to a variety of functions by the programmer. Special-purpose registers are restricted
to only specific functions. In some cases, some registers are used only to hold data
and cannot be used in the calculations of operand addresses. The length of a data
register must be long enough to hold values of most data types. Some machines
allow two contiguous registers to hold double-length values. Address registers
may be dedicated to a particular addressing mode or may be used as address general
purpose. Address registers must be long enough to hold the largest address. The
number of registers in a particular architecture affects the instruction set design.
A very small number of registers may result in an increase in memory references.
Another type of registers is used to hold processor status bits, or flags. These bits
are set by the CPU as the result of the execution of an operation. The status bits
can be tested at a later time as part of another operation.
The CPU is the part of a computer that interprets and carries out the instructions contained in the programs we write. The CPU’s main components are the register file,
ALU, and the control unit. The register file contains general-purpose and special registers. General-purpose registers may be used to hold operands and intermediate results.
The special registers may be used for memory access, sequencing, status information,
or to hold the fetched instruction during decoding and execution. Arithmetic and logical operations are performed in the ALU. Internal to the CPU, data may move from one
register to another or between registers and ALU. Data may also move between the
CPU and external components such as memory and I/O. The control unit is the component that controls the state of the instruction cycle. As long as there are instructions to
execute, the next instruction is fetched from main memory. The instruction is executed
based on the operation specified in the op-code field of the instruction. The control unit
generates signals that control the flow of data within the CPU and between the CPU and
external units such as memory and I/O. The control unit can be implemented using
hardwired or microprogramming techniques.
The assumption of needing no additional time units to recognize branch instructions and computing the target branch address is unrealistic. In typical cases, the
added hardware unit to the fetch unit will require additional time unit(s) to carry
out its task of recognizing branch instructions and computing target branch
addresses. During the extra time units needed by the hardware unit, if other instructions can be executed, then the number of extra time units needed may be reduced
and indeed may be eliminated altogether.
It is interesting to notice that a combination of dynamic and static branch prediction techniques can lead to performance improvement. An attempt to use a dynamic
branch prediction is first made, and if it is not possible, then the system can resort to
the static prediction technique.
In the first arrangement, I/O devices are assigned particular addresses, isolated
from the address space assigned to the memory. The execution of an input instruction at an input device address will cause the character stored in the input register of
that device to be transferred to a specific register in the CPU. In this case, the address and data lines from the CPU can be shared
between the memory and the I/O devices. A separate control line will have to be
used. This is because of the need for executing input and output instructions. In a
typical computer system, there exists more than one input and more than one
output device. Therefore, there is a need to have address decoder circuitry for
device identification. There is also a need for status registers for each input and
output device. The status of an input device, whether it is ready to send data to
the processor, should be stored in the status register of that device.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
I need help. I need help. I need help.
The universe (Latin: universus) is all of space and time[a] and their contents,[10] including planets, stars, galaxies, and all other forms of matter and energy. The Big Bang theory is the prevailing cosmological description of the development of the universe. According to estimation of this theory, space and time emerged together 13.799±0.021 billion years ago,[2] and the universe has been expanding ever since. While the spatial size of the entire universe is unknown,[3] the cosmic inflation equation indicates that it must have a minimum diameter of 23 trillion light years,[11] and it is possible to measure the size of the observable universe, which is approximately 93 billion light-years in diameter at the present day.

The earliest cosmological models of the universe were developed by ancient Greek and Indian philosophers and were geocentric, placing Earth at the center.[12][13] Over the centuries, more precise astronomical observations led Nicolaus Copernicus to develop the heliocentric model with the Sun at the center of the Solar System. In developing the law of universal gravitation, Isaac Newton built upon Copernicus's work as well as Johannes Kepler's laws of planetary motion and observations by Tycho Brahe.

Further observational improvements led to the realization that the Sun is one of hundreds of billions of stars in the Milky Way, which is one of a few hundred billion galaxies in the universe. Many of the stars in galaxy have planets. At the largest scale, galaxies are distributed uniformly and the same in all directions, meaning that the universe has neither an edge nor a center. At smaller scales, galaxies are distributed in clusters and superclusters which form immense filaments and voids in space, creating a vast foam-like structure.[14] Discoveries in the early 20th century have suggested that the universe had a beginning and that space has been expanding since then[15] at an increasing rate.[16]

According to the Big Bang theory, the energy and matter initially present have become less dense as the universe expanded. After an initial accelerated expansion called the inflationary epoch at around 10−32 seconds, and the separation of the four known fundamental forces, the universe gradually cooled and continued to expand, allowing the first subatomic particles and simple atoms to form. Dark matter gradually gathered, forming a foam-like structure of filaments and voids under the influence of gravity. Giant clouds of hydrogen and helium were gradually drawn to the places where dark matter was most dense, forming the first galaxies, stars, and everything else seen today.

From studying the movement of galaxies, it has been discovered that the universe contains much more matter than is accounted for by visible objects; stars, galaxies, nebulas and interstellar gas. This unseen matter is known as dark matter[17] (dark means that there is a wide range of strong indirect evidence that it exists, but we have not yet detected it directly). The ΛCDM model is the most widely accepted model of the universe. It suggests that about 69.2%±1.2% [2015] of the mass and energy in the universe is a cosmological constant (or, in extensions to ΛCDM, other forms of dark energy, such as a scalar field) which is responsible for the current expansion of space, and about 25.8%±1.1% [2015] is dark matter.[18] Ordinary ('baryonic') matter is therefore only 4.84%±0.1% [2015] of the physical universe.[18] Stars, planets, and visible gas clouds only form about 6% of the ordinary matter.[19]

There are many competing hypotheses about the ultimate fate of the universe and about what, if anything, preceded the Big Bang, while other physicists and philosophers refuse to speculate, doubting that information about prior states will ever be accessible. Some physicists have suggested various multiverse hypotheses, in which our universe might be one among many universes that likewise exist.[3][20][21]

A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations automatically. Modern computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. A computer system is a "complete" computer that includes the hardware, operating system (main software), and peripheral equipment needed and used for "full" operation. This term may also refer to a group of computers that are linked and function together, such as a computer network or computer cluster.

A broad range of industrial and consumer products use computers as control systems. Simple special-purpose devices like microwave ovens and remote controls are included, as are factory devices like industrial robots and computer-aided design, as well as general-purpose devices like personal computers and mobile devices like smartphones. Computers power the Internet, which links hundreds of millions of other computers and users.

Early computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit (IC) chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore's law), leading to the Digital Revolution during the late 20th to early 21st centuries.

Conventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved.

The physical universe is defined as all of space and time[a] (collectively referred to as spacetime) and their contents.[10] Such contents comprise all of energy in its various forms, including electromagnetic radiation and matter, and therefore planets, moons, stars, galaxies, and the contents of intergalactic space.[22][23][24] The universe also includes the physical laws that influence energy and matter, such as conservation laws, classical mechanics, and relativity.[25]

The universe is often defined as "the totality of existence", or everything that exists, everything that has existed, and everything that will exist.[25] In fact, some philosophers and scientists support the inclusion of ideas and abstract concepts—such as mathematics and logic—in the definition of the universe.[27][28][29] The word universe may also refer to concepts such as the cosmos, the world, and nature.[30][31]

The prevailing model for the evolution of the universe is the Big Bang theory.[39][40] The Big Bang model states that the earliest state of the universe was an extremely hot and dense one, and that the universe subsequently expanded and cooled. The model is based on general relativity and on simplifying assumptions such as homogeneity and isotropy of space. A version of the model with a cosmological constant (Lambda) and cold dark matter, known as the Lambda-CDM model, is the simplest model that provides a reasonably good account of various observations about the universe. The Big Bang model accounts for observations such as the correlation of distance and redshift of galaxies, the ratio of the number of hydrogen to helium atoms, and the microwave radiation background.

The initial hot, dense state is called the Planck epoch, a brief period extending from time zero to one Planck time unit of approximately 10−43 seconds. During the Planck epoch, all types of matter and all types of energy were concentrated into a dense state, and gravity—currently the weakest by far of the four known forces—is believed to have been as strong as the other fundamental forces, and all the forces may have been unified. Since the Planck epoch, space has been expanding to its present scale, with a very short but intense period of cosmic inflation believed to have occurred within the first 10−32 seconds.[41] This was a kind of expansion different from those we can see around us today. Objects in space did not physically move; instead the metric that defines space itself changed. Although objects in spacetime cannot move faster than the speed of light, this limitation does not apply to the metric governing spacetime itself. This initial period of inflation is believed to explain why space appears to be very flat, and much larger than light could travel since the start of the universe.[clarification needed]

Within the first fraction of a second of the universe's existence, the four fundamental forces had separated. As the universe continued to cool down from its inconceivably hot state, various types of subatomic particles were able to form in short periods of time known as the quark epoch, the hadron epoch, and the lepton epoch. Together, these epochs encompassed less than 10 seconds of time following the Big Bang. These elementary particles associated stably into ever larger combinations, including stable protons and neutrons, which then formed more complex atomic nuclei through nuclear fusion. This process, known as Big Bang nucleosynthesis, only lasted for about 17 minutes and ended about 20 minutes after the Big Bang, so only the fastest and simplest reactions occurred. About 25% of the protons and all the neutrons in the universe, by mass, were converted to helium, with small amounts of deuterium (a form of hydrogen) and traces of lithium. Any other element was only formed in very tiny quantities. The other 75% of the protons remained unaffected, as hydrogen nuclei.

After nucleosynthesis ended, the universe entered a period known as the photon epoch. During this period, the universe was still far too hot for matter to form neutral atoms, so it contained a hot, dense, foggy plasma of negatively charged electrons, neutral neutrinos and positive nuclei. After about 377,000 years, the universe had cooled enough that electrons and nuclei could form the first stable atoms. This is known as recombination for historical reasons; in fact electrons and nuclei were combining for the first time. Unlike plasma, neutral atoms are transparent to many wavelengths of light, so for the first time the universe also became transparent. The photons released ("decoupled") when these atoms formed can still be seen today; they form the cosmic microwave background (CMB).

As the universe expands, the energy density of electromagnetic radiation decreases more quickly than does that of matter because the energy of a photon decreases with its wavelength. At around 47,000 years, the energy density of matter became larger than that of photons and neutrinos, and began to dominate the large scale behavior of the universe. This marked the end of the radiation-dominated era and the start of the matter-dominated era.

In the earliest stages of the universe, tiny fluctuations within the universe's density led to concentrations of dark matter gradually forming. Ordinary matter, attracted to these by gravity, formed large gas clouds and eventually, stars and galaxies, where the dark matter was most dense, and voids where it was least dense. After around 100 - 300 million years,[citation needed] the first stars formed, known as Population III stars. These were probably very massive, luminous, non metallic and short-lived. They were responsible for the gradual reionization of the universe between about 200-500 million years and 1 billion years, and also for seeding the universe with elements heavier than helium, through stellar nucleosynthesis.[42] The universe also contains a mysterious energy—possibly a scalar field—called dark energy, the density of which does not change over time. After about 9.8 billion years, the universe had expanded sufficiently so that the density of matter was less than the density of dark energy, marking the beginning of the present dark-energy-dominated era.[43] In this era, the expansion of the universe is accelerating due to dark energy.

Of the four fundamental interactions, gravitation is the dominant at astronomical length scales. Gravity's effects are cumulative; by contrast, the effects of positive and negative charges tend to cancel one another, making electromagnetism relatively insignificant on astronomical length scales. The remaining two interactions, the weak and strong nuclear forces, decline very rapidly with distance; their effects are confined mainly to sub-atomic length scales.

The universe appears to have much more matter than antimatter, an asymmetry possibly related to the CP violation.[44] This imbalance between matter and antimatter is partially responsible for the existence of all matter existing today, since matter and antimatter, if equally produced at the Big Bang, would have completely annihilated each other and left only photons as a result of their interaction.[45][46] The universe also appears to have neither net momentum nor angular momentum, which follows accepted physical laws if the universe is finite. These laws are Gauss's law and the non-divergence of the stress-energy-momentum pseudotensor.[47]

According to the general theory of relativity, far regions of space may never interact with ours even in the lifetime of the universe due to the finite speed of light and the ongoing expansion of space. For example, radio messages sent from Earth may never reach some regions of space, even if the universe were to exist forever: space may expand faster than light can traverse it.[48]

The spatial region that can be observed with telescopes is called the observable universe, which depends on the location of the observer. The proper distance—the distance as would be measured at a specific time, including the present—between Earth and the edge of the observable universe is 46 billion light-years[49] (14 billion parsecs),[50] making the diameter of the observable universe about 93 billion light-years (28 billion parsecs).[49] The distance the light from the edge of the observable universe has travelled is very close to the age of the universe times the speed of light, 13.8 billion light-years (4.2×109 pc), but this does not represent the distance at any given time because the edge of the observable universe and the Earth have since moved further apart.[51] For comparison, the diameter of a typical galaxy is 30,000 light-years (9,198 parsecs), and the typical distance between two neighboring galaxies is 3 million light-years (919.8 kiloparsecs).[52] As an example, the Milky Way is roughly 100,000–180,000 light-years in diameter,[53][54] and the nearest sister galaxy to the Milky Way, the Andromeda Galaxy, is located roughly 2.5 million light-years away.[55]

Because we cannot observe space beyond the edge of the observable universe, it is unknown whether the size of the universe in its totality is finite or infinite.[3][56][57] Estimates suggest that the whole universe, if finite, must be more than 250 times larger than the observable universe.[58] Some disputed[59] estimates for the total size of the universe, if finite, reach as high as {\displaystyle 10^{10^{10^{122}}}}10^{10^{10^{122}}} megaparsecs, as implied by a suggested resolution of the No-Boundary Proposal.[60][b]

Astronomers calculate the age of the universe by assuming that the Lambda-CDM model accurately describes the evolution of the Universe from a very uniform, hot, dense primordial state to its present state and measuring the cosmological parameters which constitute the model.[citation needed] This model is well understood theoretically and supported by recent high-precision astronomical observations such as WMAP and Planck.[citation needed] Commonly, the set of observations fitted includes the cosmic microwave background anisotropy, the brightness/redshift relation for Type Ia supernovae, and large-scale galaxy clustering including the baryon acoustic oscillation feature.[citation needed] Other observations, such as the Hubble constant, the abundance of galaxy clusters, weak gravitational lensing and globular cluster ages, are generally consistent with these, providing a check of the model, but are less accurately measured at present.[citation needed] Assuming that the Lambda-CDM model is correct, the measurements of the parameters using a variety of techniques by numerous experiments yield a best value of the age of the universe as of 2015 of 13.799 ± 0.021 billion years.[2]

According to the Oxford English Dictionary, the first known use of the word "computer" was in 1613 in a book called The Yong Mans Gleanings by the English writer Richard Braithwait: "I haue [sic] read the truest computer of Times, and the best Arithmetician that euer [sic] breathed, and he reduceth thy dayes into a short number." This usage of the term referred to a human computer, a person who carried out calculations or computations. The word continued with the same meaning until the middle of the 20th century. During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts.[1] By 1943, most human computers were women.[2]

The Online Etymology Dictionary gives the first attested use of "computer" in the 1640s, meaning "one who calculates"; this is an "agent noun from compute (v.)". The Online Etymology Dictionary states that the use of the term to mean "'calculating machine' (of any type) is from 1897." The Online Etymology Dictionary indicates that the "modern use" of the term, to mean "programmable digital electronic computer" dates from "1945 under this name; [in a] theoretical [sense] from 1937, as Turing machine".[3]

Devices have been used to aid computation for thousands of years, mostly using one-to-one correspondence with fingers. The earliest counting device was probably a form of tally stick. Later record keeping aids throughout the Fertile Crescent included calculi (clay spheres, cones, etc.) which represented counts of items, probably livestock or grains, sealed in hollow unbaked clay containers.[4][5] The use of counting rods is one example.

The abacus was initially used for arithmetic tasks. The Roman abacus was developed from devices used in Babylonia as early as 2400 BC. Since then, many other forms of reckoning boards or tables have been invented. In a medieval European counting house, a checkered cloth would be placed on a table, and markers moved around on it according to certain rules, as an aid to calculating sums of money.[6]

The Antikythera mechanism is believed to be the earliest mechanical analog computer, according to Derek J. de Solla Price.[7] It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to c. 100 BC. Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later.

Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use. The planisphere was a star chart invented by Abū Rayhān al-Bīrūnī in the early 11th century.[8] The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus. A combination of the planisphere and dioptra, the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy. An astrolabe incorporating a mechanical calendar computer[9][10] and gear-wheels was invented by Abi Bakr of Isfahan, Persia in 1235.[11] Abū Rayhān al-Bīrūnī invented the first mechanical geared lunisolar calendar astrolabe,[12] an early fixed-wired knowledge processing machine[13] with a gear train and gear-wheels,[14] c. 1000 AD.

The sector, a calculating instrument used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots, was developed in the late 16th century and found application in gunnery, surveying and navigation.

The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage.

The slide rule was invented around 1620–1630 by the English clergyman William Oughtred, shortly after the publication of the concept of the logarithm. It is a hand-operated analog computer for doing multiplication and division. As slide rule development progressed, added scales provided reciprocals, squares and square roots, cubes and cube roots, as well as transcendental functions such as logarithms and exponentials, circular and hyperbolic trigonometry and other functions. Slide rules with special scales are still used for quick performance of routine calculations, such as the E6B circular slide rule used for time and distance calculations on light aircraft.

In the 1770s, Pierre Jaquet-Droz, a Swiss watchmaker, built a mechanical doll (automaton) that could write holding a quill pen. By switching the number and order of its internal wheels different letters, and hence different messages, could be produced. In effect, it could be mechanically "programmed" to read instructions. Along with two other complex machines, the doll is at the Musée d'Art et d'Histoire of Neuchâtel, Switzerland, and still operates.[15]

In 1831–1835, mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine, which, though a system of pulleys and cylinders and over, could predict the perpetual calendar for every year from AD 0 (that is, 1 BC) to AD 4000, keeping track of leap years and varying day length. The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters. It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location.

The differential analyser, a mechanical analog computer designed to solve differential equations by integration, used wheel-and-disc mechanisms to perform the integration. In 1876, Sir William Thomson had already discussed the possible construction of such calculators, but he had been stymied by the limited output torque of the ball-and-disk integrators.[16] In a differential analyzer, the output of one integrator drove the input of the next integrator, or a graphing output. The torque amplifier was the advance that allowed these machines to work. Starting in the 1920s, Vannevar Bush and others developed mechanical differential analyzers.

Charles Babbage, an English mechanical engineer and polymath, originated the concept of a programmable computer. Considered the "father of the computer",[17] he conceptualized and invented the first mechanical computer in the early 19th century. After working on his revolutionary difference engine, designed to aid in navigational calculations, in 1833 he realized that a much more general design, an Analytical Engine, was possible. The input of programs and data was to be provided to the machine via punched cards, a method being used at the time to direct mechanical looms such as the Jacquard loom. For output, the machine would have a printer, a curve plotter and a bell. The machine would also be able to punch numbers onto cards to be read in later. The Engine incorporated an arithmetic logic unit, control flow in the form of conditional branching and loops, and integrated memory, making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete.[18][19]

The machine was about a century ahead of its time. All the parts for his machine had to be made by hand – this was a major problem for a device with thousands of parts. Eventually, the project was dissolved with the decision of the British Government to cease funding. Babbage's failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow. Nevertheless, his son, Henry Babbage, completed a simplified version of the analytical engine's computing unit (the mill) in 1888. He gave a successful demonstration of its use in computing tables in 1906.

During the first half of the 20th century, many scientific computing needs were met by increasingly sophisticated analog computers, which used a direct mechanical or electrical model of the problem as a basis for computation. However, these were not programmable and generally lacked the versatility and accuracy of modern digital computers.[20] The first modern analog computer was a tide-predicting machine, invented by Sir William Thomson (later to become Lord Kelvin) in 1872. The differential analyser, a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms, was conceptualized in 1876 by James Thomson, the elder brother of the more famous Sir William Thomson.[16]

The art of mechanical analog computing reached its zenith with the differential analyzer, built by H. L. Hazen and Vannevar Bush at MIT starting in 1927. This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H. W. Nieman. A dozen of these devices were built before their obsolescence became obvious. By the 1950s, the success of digital electronic computers had spelled the end for most analog computing machines, but analog computers remained in use during the 1950s in some specialized applications such as education (slide rule) and aircraft (control systems).

Purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents, at the same time that digital calculation replaced analog. The engineer Tommy Flowers, working at the Post Office Research Station in London in the 1930s, began to explore the possible use of electronics for the telephone exchange. Experimental equipment that he built in 1934 went into operation five years later, converting a portion of the telephone exchange network into an electronic data processing system, using thousands of vacuum tubes.[20] In the US, John Vincent Atanasoff and Clifford E. Berry of Iowa State University developed and tested the Atanasoff–Berry Computer (ABC) in 1942,[28] the first "automatic electronic digital computer".[29] This design was also all-electronic and used about 300 vacuum tubes, with capacitors fixed in a mechanically rotating drum for memory.[30]

At a particular instant roughly 15 billion years ago, all the matter and energy we can observe, concentrated in a region smaller than a dime, began to expand and cool at an incredibly rapid rate. By the time the temperature had dropped to 100 million times that of the sun’s core, the forces of nature assumed their present properties, and the elementary particles known as quarks roamed freely in a sea of energy. When the universe had expanded an additional 1,000 times, all the matter we can measure filled a region the size of the solar system.

At that time, the free quarks became confined in neutrons and protons. After the universe had grown by another factor of 1,000, protons and neutrons combined to form atomic nuclei, including most of the helium and deuterium present today. All of this occurred within the first minute of the expansion. Conditions were still too hot, however, for atomic nuclei to capture electrons. Neutral atoms appeared in abundance only after the expansion had continued for 300,000 years and the universe was 1,000 times smaller than it is now. The neutral atoms then began to coalesce into gas clouds, which later evolved into stars. By the time the universe had expanded to one fifth its present size, the stars had formed groups recognizable as young galaxies.

When the universe was half its present size, nuclear reactions in stars had produced most of the heavy elements from which terrestrial planets were made. Our solar system is relatively young: it formed five billion years ago, when the universe was two thirds its present size. Over time the formation of stars has consumed the supply of gas in galaxies, and hence the population of stars is waning. Fifteen billion years from now stars like our sun will be relatively rare, making the universe a far less hospitable place for observers like us.

Our understanding of the genesis and evolution of the universe is one of the great achievements of 20th-century science. This knowledge comes from decades of innovative experiments and theories. Modern telescopes on the ground and in space detect the light from galaxies billions of light-years away, showing us what the universe looked like when it was young. Particle accelerators probe the basic physics of the high-energy environment of the early universe. Satellites detect the cosmic background radiation left over from the early stages of expansion, providing an image of the universe on the largest scales we can observe.

Our best efforts to explain this wealth of data are embodied in a theory known as the standard cosmological model or the big bang cosmology. The major claim of the theory is that in the largescale average the universe is expanding in a nearly homogeneous way from a dense early state. At present, there are no fundamental challenges to the big bang theory, although there are certainly unresolved issues within the theory itself. Astronomers are not sure, for example, how the galaxies were formed, but there is no reason to think the process did not occur within the framework of the big bang. Indeed, the predictions of the theory have survived all tests to date.

Yet the big bang model goes only so far, and many fundamental mysteries remain. What was the universe like before it was expanding? (No observation we have made allows us to look back beyond the moment at which the expansion began.) What will happen in the distant future, when the last of the stars exhaust the supply of nuclear fuel? No one knows the answers yet.

Our universe may be viewed in many lights—by mystics, theologians, philosophers or scientists. In science we adopt the plodding route: we accept only what is tested by experiment or observation. Albert Einstein gave us the now well-tested and accepted Theory of General Relativity, which establishes the relations between mass, energy, space and time. Einstein showed that a homogeneous distribution of matter in space fits nicely with his theory. He assumed without discussion that the universe is static, unchanging in the large-scale average [see “How Cosmology Became a Science,” by Stephen G. Brush; SCIENTIFIC AMERICAN, August 1992].

In 1922 the Russian theorist Alexander A. Friedmann realized that Einstein’s universe is unstable; the slightest perturbation would cause it to expand or contract. At that time, Vesto M. Slipher of Lowell Observatory was collecting the first evidence that galaxies are actually moving apart. Then, in 1929, the eminent astronomer Edwin P. Hubble showed that the rate a galaxy is moving away from us is roughly proportional to its distance from us.

The existence of an expanding universe implies that the cosmos has evolved from a dense concentration of matter into the present broadly spread distribution of galaxies. Fred Hoyle, an English cosmologist, was the first to call this process the big bang. Hoyle intended to disparage the theory, but the name was so catchy it gained popularity. It is somewhat misleading, however, to describe the expansion as some type of explosion of matter away from some particular point in space.

That is not the picture at all: in Einstein’s universe the concept of space and the distribution of matter are intimately linked; the observed expansion of the system of galaxies reveals the unfolding of space itself. An essential feature of the theory is that the average density in space declines as the universe expands; the distribution of matter forms no observable edge. In an explosion the fastest particles move out into empty space, but in the big bang cosmology, particles uniformly fill all space. The expansion of the universe has had little influence on the size of galaxies or even clusters of galaxies that are bound by gravity; space is simply opening up between them. In this sense, the expansion is similar to a rising loaf of raisin bread. The dough is analogous to space, and the raisins, to clusters of galaxies. As the dough expands, the raisins move apart. Moreover, the speed with which any two raisins move apart is directly and positively related to the amount of dough separating them.

The evidence for the expansion of the universe has been accumulating for some 60 years. The first important clue is the redshift. A galaxy emits or absorbs some wavelengths of light more strongly than others. If the galaxy is moving away from us, these emission and absorption features are shifted to longer wavelengths—that is, they become redder as the recession velocity increases. This phenomenon is known as the redshift.

Hubble’s measurements indicated that the redshift of a distant galaxy is greater than that of one closer to the earth. This relation, now known as Hubble’s law, is just what one would expect in a uniformly expanding universe. Hubble’s law says the recession velocity of a galaxy is equal to its distance multiplied by a quantity called Hubble’s constant. The redshift effect in nearby galaxies is relatively subtle, requiring good instrumentation to detect it. In contrast, the redshift of very distant objects—radio galaxies and quasars—is an awesome phenomenon; some appear to be moving away at greater than 90 percent of the speed of light.

Hubble contributed to another crucial part of the picture. He counted the number of visible galaxies in different directions in the sky and found that they appear to be rather uniformly distributed. The value of Hubble’s constant seemed to be the same in all directions, a necessary consequence of uniform expansion. Modern surveys confirm the fundamental tenet that the universe is homogeneous on large scales. Although maps of the distribution of the nearby galaxies display clumpiness, deeper surveys reveal considerable uniformity.

The Milky Way, for instance, resides in a knot of two dozen galaxies; these in turn are part of a complex of galaxies that protrudes from the so-called local supercluster. The hierarchy of clustering has been traced up to dimensions of about 500 million light-years. The fluctuations in the average density of matter diminish as the scale of the structure being investigated increases. In maps that cover distances that reach close to the observable limit, the average density of matter changes by less than a tenth of a percent.

To test Hubble’s law, astronomers need to measure distances to galaxies. One method for gauging distance is to observe the apparent brightness of a galaxy. If one galaxy is four times fainter in the night sky than an otherwise comparable galaxy, then it can be estimated to be twice as far away. This expectation has now been tested over the whole of the visible range of distances.

Some critics of the theory have pointed out that a galaxy that appears to be smaller and fainter might not actually be more distant. Fortunately, there is a direct indication that objects whose redshifts are larger really are more distant. The evidence comes from observations of an effect known as gravitational lensing. An object as massive and compact as a galaxy can act as a crude lens, producing a distorted, magnified image (or even many images) of any background radiation source that lies behind it. Such an object does so by bending the paths of light rays and other electromagnetic radiation. So if a galaxy sits in the line of sight between the earth and some distant object, it will bend the light rays from the object so that they are observable [see “Gravitational Lenses,” by Edwin L. Turner; SCIENTIFIC AMERICAN, July 1988]. During the past decade, astronomers have discovered more than a dozen gravitational lenses. The object behind the lens is always found to have a higher redshift than the lens itself, confirming the qualitative prediction of Hubble’s law.

Hubble’s law has great significance not only because it describes the expansion of the universe but also because it can be used to calculate the age of the cosmos. To be precise, the time elapsed since the big bang is a function of the present value of Hubble’s constant and its rate of change. Astronomers have determined the approximate rate of the expansion, but no one has yet been able to measure the second value precisely.

Still, one can estimate this quantity from knowledge of the universe’s average density. One expects that because gravity exerts a force that opposes expansion, galaxies would tend to move apart more slowly now than they did in the past. The rate of change in expansion is therefore related to the gravitational pull of the universe set by its average density. If the density is that of just the visible material in and around galaxies, the age of the universe probably lies between 12 and 20 billion years. (The range allows for the uncertainty in the rate of expansion.)

Yet many researchers believe the density is greater than this minimum value. So-called dark matter would make up the difference. A strongly defended argument holds that the universe is just dense enough that in the remote future the expansion will slow almost to zero. Under this assumption, the age of the universe decreases to the range of seven to 13 billion years.

Modern machines are complex systems that consist of structural elements, mechanisms and control components and include interfaces for convenient use. Examples include: a wide range of vehicles, such as automobiles, boats and airplanes; appliances in the home and office, including computers, building air handling and water handling systems; as well as farm machinery, machine tools and factory automation systems and robots.

An automaton (/ɔːˈtɒmətən/; plural: automata or automatons) is a relatively self-operating machine, or control mechanism designed to automatically follow a predetermined sequence of operations, or respond to predetermined instructions.[1] Some automata, such as bellstrikers in mechanical clocks, are designed to give the illusion to the casual observer that they are operating under their own power. Since long ago, the term is commonly associated with automated puppets that resemble moving humans or animals, built to impress and/or to entertain people.

The Milky Way[a] is the galaxy that includes our Solar System, with the name describing the galaxy's appearance from Earth: a hazy band of light seen in the night sky formed from stars that cannot be individually distinguished by the naked eye. The term Milky Way is a translation of the Latin via lactea, from the Greek γαλακτικός κύκλος (galaktikos kýklos, "milky circle").[19][20][21] From Earth, the Milky Way appears as a band because its disk-shaped structure is viewed from within. Galileo Galilei first resolved the band of light into individual stars with his telescope in 1610. Until the early 1920s, most astronomers thought that the Milky Way contained all the stars in the Universe.[22] Following the 1920 Great Debate between the astronomers Harlow Shapley and Heber Curtis,[23] observations by Edwin Hubble showed that the Milky Way is just one of many galaxies.

The Milky Way is a barred spiral galaxy with an estimated visible diameter of 100,000–200,000 light-years. Recent simulations suggest that a dark matter disk, also containing some visible stars, may extend up to a diameter of almost 2 million light-years.[11][12] The Milky Way has several satellite galaxies and is part of the Local Group of galaxies, which form part of the Virgo Supercluster, which is itself a component of the Laniakea Supercluster.[24][25]

It is estimated to contain 100–400 billion stars[26][27] and at least that number of planets.[28][29] The Solar System is located at a radius of about 27,000 light-years from the Galactic Center,[2] on the inner edge of the Orion Arm, one of the spiral-shaped concentrations of gas and dust. The stars in the innermost 10,000 light-years form a bulge and one or more bars that radiate from the bulge. The galactic center is an intense radio source known as Sagittarius A*, a supermassive black hole of 4.100 (± 0.034) million solar masses. Stars and gases at a wide range of distances from the Galactic Center orbit at approximately 220 kilometers per second. The constant rotation speed contradicts the laws of Keplerian dynamics and suggests that much (about 90%)[30][31] of the mass of the Milky Way is invisible to telescopes, neither emitting nor absorbing electromagnetic radiation. This conjectural mass has been termed "dark matter".[32] The rotational period is about 240 million years at the radius of the Sun.[16] The Milky Way as a whole is moving at a velocity of approximately 600 km per second with respect to extragalactic frames of reference. The oldest stars in the Milky Way are nearly as old as the Universe itself and thus probably formed shortly after the Dark Ages of the Big Bang.[33]

The Milky Way is visible from Earth as a hazy band of white light, some 30° wide, arching the night sky.[34] In night sky observing, although all the individual naked-eye stars in the entire sky are part of the Milky Way Galaxy, the term "Milky Way" is limited to this band of light.[35][36] The light originates from the accumulation of unresolved stars and other material located in the direction of the galactic plane. Brighter regions around the band appear as soft visual patches known as star clouds. The most conspicuous of these is the Large Sagittarius Star Cloud, a portion of the central bulge of the galaxy.[37] Dark regions within the band, such as the Great Rift and the Coalsack, are areas where interstellar dust blocks light from distant stars. The area of sky that the Milky Way obscures is called the Zone of Avoidance.

The Milky Way has a relatively low surface brightness. Its visibility can be greatly reduced by background light, such as light pollution or moonlight. The sky needs to be darker than about 20.2 magnitude per square arcsecond in order for the Milky Way to be visible.[38] It should be visible if the limiting magnitude is approximately +5.1 or better and shows a great deal of detail at +6.1.[39] This makes the Milky Way difficult to see from brightly lit urban or suburban areas, but very prominent when viewed from rural areas when the Moon is below the horizon.[b] Maps of artificial night sky brightness show that more than one-third of Earth's population cannot see the Milky Way from their homes due to light pollution.[40]

As viewed from Earth, the visible region of the Milky Way's galactic plane occupies an area of the sky that includes 30 constellations.[41] The Galactic Center lies in the direction of Sagittarius, where the Milky Way is brightest. From Sagittarius, the hazy band of white light appears to pass around to the galactic anticenter in Auriga. The band then continues the rest of the way around the sky, back to Sagittarius, dividing the sky into two roughly equal hemispheres.

The galactic plane is inclined by about 60° to the ecliptic (the plane of Earth's orbit). Relative to the celestial equator, it passes as far north as the constellation of Cassiopeia and as far south as the constellation of Crux, indicating the high inclination of Earth's equatorial plane and the plane of the ecliptic, relative to the galactic plane. The north galactic pole is situated at right ascension 12h 49m, declination +27.4° (B1950) near β Comae Berenices, and the south galactic pole is near α Sculptoris. Because of this high inclination, depending on the time of night and year, the Milky Way arch may appear relatively low or relatively high in the sky. For observers from latitudes approximately 65° north to 65° south, the Milky Way passes directly overhead twice a day.

The Milky Way is the second-largest galaxy in the Local Group (after the Andromeda Galaxy), with its stellar disk approximately 170,000–200,000 light-years (52–61 kpc) in diameter and, on average, approximately 1,000 ly (0.3 kpc) thick.[13][14] The Milky Way is approximately 890 billion to 1.54 trillion times the mass of the Sun.[30][31] To compare the relative physical scale of the Milky Way, if the Solar System out to Neptune were the size of a US quarter (24.3 mm (0.955 in)), the Milky Way would be approximately the size of the contiguous United States.[42] There is a ring-like filament of stars rippling above and below the relatively flat galactic plane, wrapping around the Milky Way at a diameter of 150,000–180,000 light-years (46–55 kpc),[43] which may be part of the Milky Way itself.[44]

The Milky Way contains between 100-400 billion stars[59][60] and at least that many planets.[61] An exact figure would depend on counting the number of very-low-mass stars, which are difficult to detect, especially at distances of more than 300 ly (90 pc) from the Sun. As a comparison, the neighboring Andromeda Galaxy contains an estimated one trillion (1012) stars.[62] The Milky Way may contain ten billion white dwarfs, a billion neutron stars, and a hundred million stellar black holes.[c][63][64][65][66] Filling the space between the stars is a disk of gas and dust called the interstellar medium. This disk has at least a comparable extent in radius to the stars,[67] whereas the thickness of the gas layer ranges from hundreds of light-years for the colder gas to thousands of light-years for warmer gas.[68][69]

The disk of stars in the Milky Way does not have a sharp edge beyond which there are no stars. Rather, the concentration of stars decreases with distance from the center of the Milky Way. For reasons that are not understood, beyond a radius of roughly 40,000 ly (13 kpc) from the center, the number of stars per cubic parsec drops much faster with radius.[70] Surrounding the galactic disk is a spherical Galactic Halo of stars and globular clusters that extends farther outward, but is limited in size by the orbits of two Milky Way satellites, the Large and Small Magellanic Clouds, whose closest approach to the Galactic Center is about 180,000 ly (55 kpc).[71] At this distance or beyond, the orbits of most halo objects would be disrupted by the Magellanic Clouds. Hence, such objects would probably be ejected from the vicinity of the Milky Way. The integrated absolute visual magnitude of the Milky Way is estimated to be around −20.9.[72][73][d]

Both gravitational microlensing and planetary transit observations indicate that there may be at least as many planets bound to stars as there are stars in the Milky Way,[28][74] and microlensing measurements indicate that there are more rogue planets not bound to host stars than there are stars.[75][76] The Milky Way contains at least one planet per star, resulting in 100–400 billion planets, according to a January 2013 study of the five-planet star system Kepler-32 by the Kepler space observatory.[29] A different January 2013 analysis of Kepler data estimated that at least 17 billion Earth-sized exoplanets reside in the Milky Way.[77] On November 4, 2013, astronomers reported, based on Kepler space mission data, that there could be as many as 40 billion Earth-sized planets orbiting in the habitable zones of Sun-like stars and red dwarfs within the Milky Way.[78][79][80] 11 billion of these estimated planets may be orbiting Sun-like stars.[81] The nearest exoplanet may be 4.2 light-years away, orbiting the red dwarf Proxima Centauri, according to a 2016 study.[82] Such Earth-sized planets may be more numerous than gas giants.[28] Besides exoplanets, "exocomets", comets beyond the Solar System, have also been detected and may be common in the Milky Way.[83] More recently, in November 2020, over 300 million habitable exoplanets are estimated to exist in the Milky Way Galaxy.[84]

The Milky Way consists of a bar-shaped core region surrounded by a warped disk of gas, dust and stars.[85][86] The mass distribution within the Milky Way closely resembles the type Sbc in the Hubble classification, which represents spiral galaxies with relatively loosely wound arms.[3] Astronomers first began to conjecture that the Milky Way is a barred spiral galaxy, rather than an ordinary spiral galaxy, in the 1960s.[87][88][89] These conjectures were confirmed by the Spitzer Space Telescope observations in 2005[90] that showed the Milky Way's central bar to be larger than previously thought.In Meteorologica, Aristotle (384–322 BC) states that the Greek philosophers Anaxagoras (c. 500–428 BC) and Democritus (460–370 BC) proposed that the Milky Way is the glow of stars not directly visible due to Earth's shadow, while other stars receive their light from the Sun (but have their glow obscured by solar rays).[217] Aristotle himself believed that the Milky Way was part of the Earth's upper atmosphere (along with the stars), and that it was a byproduct of stars burning that did not dissipate because of its outermost location in the atmosphere (composing its great circle).[218][219] The Neoplatonist philosopher Olympiodorus the Younger (c. 495–570 AD) criticized this view, arguing that if the Milky Way were sublunary, it should appear different at different times and places on Earth, and that it should have parallax, which it does not. In his view, the Milky Way is celestial. This idea would be influential later in the Islamic world.[220]

The Persian astronomer Abū Rayhān al-Bīrūnī (973–1048) proposed that the Milky Way is "a collection of countless fragments of the nature of nebulous stars".[221] The Andalusian astronomer Avempace (d 1138) proposed the Milky Way to be made up of many stars but appears to be a continuous image due to the effect of refraction in Earth's atmosphere, citing his observation of a conjunction of Jupiter and Mars in 1106 or 1107 as evidence.[219] According to Jamil Ragep, the Persian astronomer Naṣīr al-Dīn al-Ṭūsī (1201–1274) in his Tadhkira writes: "The Milky Way, i.e. the Galaxy, is made up of a very large number of small, tightly clustered stars, which, on account of their concentration and smallness, seem to be cloudy patches. Because of this, it was likened to milk in color."[222] Ibn Qayyim Al-Jawziyya (1292–1350) proposed that the Milky Way is "a myriad of tiny stars packed together in the sphere of the fixed stars" and that these stars are larger than planets.[223]

Proof of the Milky Way consisting of many stars came in 1610 when Galileo Galilei used a telescope to study the Milky Way and discovered that it is composed of a huge number of faint stars.[224][225] In a treatise in 1755, Immanuel Kant, drawing on earlier work by Thomas Wright,[226] speculated (correctly) that the Milky Way might be a rotating body of a huge number of stars, held together by gravitational forces akin to the Solar System but on much larger scales.[227] The resulting disk of stars would be seen as a band on the sky from our perspective inside the disk. Wright and Kant also conjectured that some of the nebulae visible in the night sky might be separate "galaxies" themselves, similar to our own. Kant referred to both the Milky Way and the "extragalactic nebulae" as "island universes", a term still current up to the 1930s.[228][229][230]

The first attempt to describe the shape of the Milky Way and the position of the Sun within it was carried out by William Herschel in 1785 by carefully counting the number of stars in different regions of the visible sky. He produced a diagram of the shape of the Milky Way with the Solar System close to the center.[231]

In 1845, Lord Rosse constructed a new telescope and was able to distinguish between elliptical and spiral-shaped nebulae. He also managed to make out individual point sources in some of these nebulae, lending credence to Kant's earlier conjecture.[232][233]

The Internet (or internet) is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.

The origins of the Internet date back to the development of packet switching and research commissioned by the United States Department of Defense in the 1960s to enable time-sharing of computers.[1] The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1970s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks.[2] The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet,[3] and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia in the 1980s, commercialization incorporated its services and technologies into virtually every aspect of modern life.

Most traditional communication media, including telephony, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephony, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking services. Online shopping has grown exponentially for major retailers, small businesses, and entrepreneurs, as it enables firms to extend their "brick and mortar" presence to serve a larger market or even sell goods and services entirely online. Business-to-business and financial services on the Internet affect supply chains across entire industries.

The Internet has no single centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies.[4] The overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise.[5] In November 2006, the Internet was included on USA Today's list of New Seven Wonders.[6]

The word internetted was used as early as 1849, meaning interconnected or interwoven.[7] The word Internet was used in 1974 as the shorthand form of Internetwork.[8] Today, the term Internet most commonly refers to the global system of interconnected computer networks, though it may also refer to any group of smaller networks.[9]

When it came into common use, most publications treated the word Internet as a capitalized proper noun; this has become less common.[9] This reflects the tendency in English to capitalize new terms and move to lowercase as they become familiar.[9][10] The word is sometimes still capitalized to distinguish the global internet from smaller networks, though many publications, including the AP Stylebook, recommend the lowercase form in every case.[9][10] In 2016, the Oxford English Dictionary found that, based on a study of around 2.5 billion printed and online sources, "Internet" was capitalized in 54% of cases.[11]

The terms internet and World Wide Web are often used interchangeably; it is common to speak of "going on the Internet" when using a web browser to view web pages. However, the World Wide Web or the Web is only one of a large number of Internet services,[12] a collection of documents (web pages) and other web resources, linked by hyperlinks and URLs.[13]

In the 1960s, the Advanced Research Projects Agency (ARPA) of the United States Department of Defense funded research into time-sharing of computers.[14][15][16] Research into packet switching, one of the fundamental Internet technologies, started in the work of Paul Baran in the early 1960s and, independently, Donald Davies in 1965.[1][17] After the Symposium on Operating Systems Principles in 1967, packet switching from the proposed NPL network was incorporated into the design for the ARPANET and other resource sharing networks such as the Merit Network and CYCLADES, which were developed in the late 1960s and early 1970s.[18]

ARPANET development began with two network nodes which were interconnected between the Network Measurement Center at the University of California, Los Angeles (UCLA) Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock, and the NLS system at SRI International (SRI) by Douglas Engelbart in Menlo Park, California, on 29 October 1969.[19] The third site was the Culler-Fried Interactive Mathematics Center at the University of California, Santa Barbara, followed by the University of Utah Graphics Department. In a sign of future growth, 15 sites were connected to the young ARPANET by the end of 1971.[20][21] These early years were documented in the 1972 film Computer Networks: The Heralds of Resource Sharing.[22]

Early international collaborations for the ARPANET were rare. Connections were made in 1973 to the Norwegian Seismic Array (NORSAR) via a satellite station in Tanum, Sweden, and to Peter Kirstein's research group at University College London which provided a gateway to British academic networks.[23][24] The ARPA projects and international working groups led to the development of various protocols and standards by which multiple separate networks could become a single network or "a network of networks".[25] In 1974, Vint Cerf and Bob Kahn used the term internet as a shorthand for internetwork in RFC 675,[26] and later RFCs repeated this use.[27] Cerf and Khan credit Louis Pouzin with important influences on TCP/IP design.[28] Commercial PTT providers were concerned with developing X.25 public data networks.[29]

Access to the ARPANET was expanded in 1981 when the National Science Foundation (NSF) funded the Computer Science Network (CSNET). In 1982, the Internet Protocol Suite (TCP/IP) was standardized, which permitted worldwide proliferation of interconnected networks. TCP/IP network access expanded again in 1986 when the National Science Foundation Network (NSFNet) provided access to supercomputer sites in the United States for researchers, first at speeds of 56 kbit/s and later at 1.5 Mbit/s and 45 Mbit/s.[30] The NSFNet expanded into academic and research organizations in Europe, Australia, New Zealand and Japan in 1988–89.[31][32][33][34] Although other network protocols such as UUCP had global reach well before this time, this marked the beginning of the Internet as an intercontinental network. Commercial Internet service providers (ISPs) emerged in 1989 in the United States and Australia.[35] The ARPANET was decommissioned in 1990.[36]

The communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture. As with any computer network, the Internet physically consists of routers, media (such as cabling and radio links), repeaters, modems etc. However, as an example of internetworking, many of the network nodes are not necessarily internet equipment per se, the internet packets are carried by other full-fledged networking protocols with the Internet acting as a homogeneous networking standard, running across heterogeneous hardware, with the packets guided to their destinations by IP routers.

Internet service providers (ISPs) establish the worldwide connectivity between individual networks at various levels of scope. End-users who only access the Internet when needed to perform a function or obtain information, represent the bottom of the routing hierarchy. At the top of the routing hierarchy are the tier 1 networks, large telecommunication companies that exchange traffic directly with each other via very high speed fibre optic cables and governed by peering agreements. Tier 2 and lower-level networks buy Internet transit from other providers to reach at least some parties on the global Internet, though they may also engage in peering. An ISP may use a single upstream provider for connectivity, or implement multihoming to achieve redundancy and load balancing. Internet exchange points are major traffic exchanges with physical connections to multiple ISPs. Large organizations, such as academic institutions, large enterprises, and governments, may perform the same function as ISPs, engaging in peering and purchasing transit on behalf of their internal networks. Research networks tend to interconnect with large subnetworks such as GEANT, GLORIAD, Internet2, and the UK's national research and education network, JANET.

Common methods of Internet access by users include dial-up with a computer modem via telephone circuits, broadband over coaxial cable, fiber optics or copper wires, Wi-Fi, satellite, and cellular telephone technology (e.g. 3G, 4G). The Internet may often be accessed from computers in libraries and Internet cafes. Internet access points exist in many public places such as airport halls and coffee shops. Various terms are used, such as public Internet kiosk, public access terminal, and Web payphone. Many hotels also have public terminals that are usually fee-based. These terminals are widely accessed for various usages, such as ticket booking, bank deposit, or online payment. Wi-Fi provides wireless access to the Internet via local computer networks. Hotspots providing such access include Wi-Fi cafes, where users need to bring their own wireless devices such as a laptop or PDA. These services may be free to all, free to customers only, or fee-based.

Grassroots efforts have led to wireless community networks. Commercial Wi-Fi services that cover large areas are available in many cities, such as New York, London, Vienna, Toronto, San Francisco, Philadelphia, Chicago and Pittsburgh, where the Internet can then be accessed from places such as a park bench.[59] Experiments have also been conducted with proprietary mobile wireless networks like Ricochet, various high-speed data services over cellular networks, and fixed wireless services. Modern smartphones can also access the Internet through the cellular carrier network. For Web browsing, these devices provide applications such as Google Chrome, Safari, and Firefox and a wide variety of other Internet software may be installed from app-stores. Internet usage by mobile and tablet devices exceeded desktop worldwide for the first time in October 2016.[60]